# LLM-jailbreak-defense

## 白盒攻击
* Adversarial Attack
  * [加后缀调梯度实现通用模板（GCG）](https://arxiv.org/pdf/2307.15043.pdf)
  * [利用图片实现越狱（JAILBREAK IN PIECES: COMPOSITIONAL ADVERSARIAL ATTACKS ON MULTI-MODAL LANGUAGE MODELS）](https://arxiv.org/pdf/2307.14539.pdf)
* 调参多次采样
  * [CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXPLOITING GENERATION](https://arxiv.org/pdf/2310.06987.pdf)
  * [ProMan](https://arxiv.org/pdf/2310.01581.pdf)

### 黑盒攻击
* 越狱Prompt模板设计
  * [公式代码模板（Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks）](https://arxiv.org/pdf/2302.05733.pdf)
  * [多步越狱prompts（MJP）](https://arxiv.org/pdf/2304.05197.pdf)
  * [利用编码能力（CIPHERCHAT）](https://arxiv.org/pdf/2308.06463.pdf)
* 大模型自动越狱prompt生成
  * [修剪攻击树（TAP）](https://arxiv.org/pdf/2312.02119.pdf)
  * [训练大模型生成越狱prompt（MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots）](https://arxiv.org/pdf/2307.08715.pdf)
  * [利用遗传算法实现自动越狱(OPEN SESAME! UNIVERSAL BLACK BOX JAILBREAKING OF LARGE LANGUAGE MODEL)](https://arxiv.org/pdf/2309.01446.pdf)
